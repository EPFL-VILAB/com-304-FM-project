def forward(self, x: torch.Tensor, mask : Optional[torch.Tensor] = None) -> torch.Tensor:
        B, L, D = x.shape # Batch size, sequence length, and dimension

        # TODO: Compute the keys K, queries Q, and values V from x. Each should be of shape [B num_heads L head_dim].
        q, k, v = self.qkv(x).chunk(3, dim= -1)
        # q = self.q(x).reshape(B,self.num_heads,L,-1)
        # k = self.k(x).reshape(B,self.num_heads,L,-1)
        # v = self.v(x).reshape(B,self.num_heads,L,-1)

        # DEBUG TOUT REMETTRE BIEN ICI 
        # # TODO: Compute the attention matrix (pre softmax) and scale it by 1/sqrt(d_k). It should be of shape [B num_heads L L].
        # # Hint: Use the already defined self.scale
        # attn = q @ k.transpose(-2,-1) * self.scale

        if mask is not None:
            mask = rearrange(mask, "b n m -> b 1 n m") # Unsqueeze for multi-head attention
            # TODO: Apply the optional attention mask. Wherever the mask is False, replace the attention 
            # matrix value by negative infinity â†’ zero attention weight after softmax.
            # attn = attn.masked_fill_(mask == False, - float('inf'))

        # # TODO: Compute the softmax over the last dimension
        # # attn = self.softmax(attn)
        attn = F.scaled_dot_product_attention(q,k,v,attn_mask=mask, scale = self.scale)
        # attn = F.softmax(attn,dim=-1) #debug 

        # TODO: Weight the values V by the attention matrix and concatenate the different attention heads
        # Make sure to reshape the output to the original shape of x, i.e. [B L D]
        # x = torch.matmul(attn,v).reshape(B,L,D)
        x = attn.transpose(1, 2).reshape(B, L, D)

        # Output projection
        x = self.attn_out_proj(x)
        return x