{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd60a039-9bbe-4b7e-a649-52f126112b46",
   "metadata": {},
   "source": [
    "# **Exploring Multi-Modal Retrieval with 4M**  \n",
    "\n",
    "In this second part of the 4M tutorial, you will learn how to use the 4M model for multi-modal retrieval.  \n",
    "\n",
    "The key idea behind multi-modal retrieval is to map one or multiple input modalities (such as image, text, or semantic segmentation) into a shared latent feature space. By computing distances between these feature representations, we can find examples that exhibit high similarity.  \n",
    "\n",
    "In this notebook, we achieve multi-modal retrieval by mapping input modalities into the **feature space** of [DINOv2](https://arxiv.org/abs/2304.07193) or [ImageBind](https://arxiv.org/abs/2305.05665). These models are known for learning powerful representations and were used as one of the modalities during **4M pretraining**, allowing us to leverage them as target modalities for retrieval by predicting them from arbitrary inputs.\n",
    "\n",
    "Follow the steps in this notebook to understand the **multi-modal retrieval process** and complete the hands-on practice at the end.\n",
    "\n",
    "The setup in this notebook is similar to the one from the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d31266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3adaee",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision.utils import make_grid\n",
    "from tokenizers import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "except:\n",
    "    print('Please install faiss via pip install faiss-gpu to perform retrieval.')\n",
    "\n",
    "from fourm.models.fm import FM\n",
    "from fourm.vq.vqvae import VQVAE, DiVAE\n",
    "from fourm.models.generate import GenerationSampler, build_chained_generation_schedules, init_empty_target_modality, init_full_input_modality, custom_text\n",
    "from fourm.data.modality_transforms import RGBTransform\n",
    "from fourm.data.modality_info import MODALITY_INFO\n",
    "from fourm.data.modality_transforms import MetadataTransform\n",
    "from fourm.utils.plotting_utils import decode_dict, visualize_bboxes, plot_text_in_square, decode_tok_depth, decode_tok_semseg\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0efa72-20a6-4944-b7ef-b0064ad9efd8",
   "metadata": {},
   "source": [
    "### Load tokenizers\n",
    "\n",
    "To encode modalities into discrete tokens, and in turn decode the tokens that 4M predicts, we use modality-specific tokenizers. We can easily load them from [Hugging Face hub](https://huggingface.co/EPFL-VILAB) with the below lines.\n",
    "\n",
    "The tokenizer checkpoint names are formated as: `f'4M_tokenizers_{modality}_{vocab_size}_{min_res}-{max–res}'`.\n",
    "All tokenizers here are trained to work on resolutions between 224 and 448, in steps of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f6c3e-6480-48c7-8f66-d19076116b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tok = Tokenizer.from_file('toks/text_tokenizer_4m_wordpiece_30k.json')\n",
    "\n",
    "toks = {\n",
    "    'tok_depth': DiVAE.from_pretrained('EPFL-VILAB/4M_tokenizers_depth_8k_224-448').eval().to(device),\n",
    "    'tok_semseg': VQVAE.from_pretrained('EPFL-VILAB/4M_tokenizers_semseg_4k_224-448').eval().to(device),\n",
    "    'tok_dinov2_global': VQVAE.from_pretrained('EPFL-VILAB/4M_tokenizers_DINOv2-B14-global_8k_16_224').eval().to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057ac15-339e-4517-abc0-0badb4f846f3",
   "metadata": {},
   "source": [
    "### Load a 4M-21 model\n",
    "\n",
    "Let's load a 4M-21 XL model that was trained on 21 modalities, including RGB, depth, surface normals, semantic segmentation, SAM instances, Canny & SAM edges, 3D human poses, CLIP-B/16 features, DINOv2-B/14 features, ImageBind-H/14 features, captions, metadata, color palette and bounding boxes. It can take any combination of those modalities as input, and can predict all of them. We wrap the model in a `GenerationSampler` which provides inference utilities.\n",
    "\n",
    "Please see [official repo's](https://github.com/apple/ml-4m) `README.md` for all available 4M and tokenizer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eecb541-a538-4f80-a6de-c87bce17fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = FM.from_pretrained('EPFL-VILAB/4M-21_B').eval().to(device)\n",
    "sampler = GenerationSampler(fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852eb487-928c-4d2c-8b47-fa0f921d855e",
   "metadata": {},
   "source": [
    "### RGB → DINOv2 global embedding\n",
    "\n",
    "Given an RGB input, predict DINOv2. Inference/generation is done by providing a _generation schedule_ to the `GenerationSampler`. \n",
    "A _generation schedule_ specifies the order of modalities to generate, and for each it contains the generation parameters like number of steps or temperature.\n",
    "We provide a convenience function `build_chained_generation_schedules` that allows for building arbitrary chained generation schedules.\n",
    "We call it chained generation, since every newly generated output is looped back into the input and serves as conditioning for subsequently generated modalities.\n",
    "This enables generating multiple modalities that are all consistent with each other, which is mostly important when the conditioning is underspecified.\n",
    "Please see the generation notebook and the paper for more details on chained generation.\n",
    "\n",
    "The `build_chained_generation_schedules` function takes several inputs. \n",
    "Please see the generation README in [`README_GENERATION.md`](https://github.com/apple/ml-4m/blob/main/README_GENERATION.md) for detailed information on each of them, and tips on how to set them.\n",
    "In the following, we give an RGB image as input (pixels not tokens), and predict DINOv2 global embeddings from it. Note that while this demo uses an RGB image as input to predict the global embeddings, any other subset of modalities can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15e048-cfd9-40bc-902f-79a8fc7aaf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_domains = ['rgb@224']\n",
    "target_domains = ['tok_dinov2_global'] \n",
    "tokens_per_target = [16]\n",
    "autoregression_schemes = ['roar']\n",
    "decoding_steps = [1]\n",
    "token_decoding_schedules = ['linear']\n",
    "temps = [0.1]\n",
    "temp_schedules = ['constant'] \n",
    "cfg_scales = [1.0]\n",
    "cfg_schedules = ['constant'] \n",
    "cfg_grow_conditioning = True\n",
    "top_p, top_k = 0.8, 0.0\n",
    "\n",
    "schedule = build_chained_generation_schedules(\n",
    "    cond_domains=cond_domains, target_domains=target_domains, tokens_per_target=tokens_per_target, autoregression_schemes=autoregression_schemes, \n",
    "    decoding_steps=decoding_steps, token_decoding_schedules=token_decoding_schedules, temps=temps, temp_schedules=temp_schedules,\n",
    "    cfg_scales=cfg_scales, cfg_schedules=cfg_schedules, cfg_grow_conditioning=cfg_grow_conditioning, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a5d82-688c-490b-a7f8-d7d69242aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image from the specified URL and preprocess it\n",
    "image_url = 'https://storage.googleapis.com/four_m_site/images/demo_rgb.png'\n",
    "!curl $image_url --output input.jpg\n",
    "\n",
    "rgb_transform = RGBTransform(imagenet_default_mean_and_std=True)\n",
    "img_pil = rgb_transform.load('./input.jpg')\n",
    "img_pil = rgb_transform.preprocess(img_pil)\n",
    "img_pil = center_crop(img_pil, (min(img_pil.size), min(img_pil.size))).resize((224,224))\n",
    "img = rgb_transform.postprocess(img_pil).unsqueeze(0).to(device)\n",
    "img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download retrieval data for depth and semantic segmentation (3.8G)\n",
    "retrieval_data_url = 'https://datasets.epfl.ch/vilab/retrieval_data_4m/retrieval_data_4m.tar.gz'\n",
    "!curl $retrieval_data_url --output retrieval_data_4m.tar.gz\n",
    "!tar -xvf retrieval_data_4m.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14597ecf-538a-436c-8c8f-f44928537320",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_sample = {\n",
    "    'rgb@224': {\n",
    "        'tensor': img, # Batched tensor\n",
    "        'input_mask': torch.zeros(1, 196, dtype=torch.bool, device=device), # False = used as input, True = ignored\n",
    "        'target_mask': torch.ones(1, 196, dtype=torch.bool, device=device), # False = predicted as target, True = ignored\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize target modalities\n",
    "for target_mod, ntoks in zip(target_domains, tokens_per_target):\n",
    "    batched_sample = init_empty_target_modality(batched_sample, MODALITY_INFO, target_mod, 1, ntoks, device)\n",
    "    \n",
    "# Initialize input modalities\n",
    "for cond_mod in cond_domains:\n",
    "    batched_sample = init_full_input_modality(batched_sample, MODALITY_INFO, cond_mod, device, eos_id=text_tok.token_to_id(\"[EOS]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63cdb7a-77f3-4d3b-a197-c988a823fc2f",
   "metadata": {},
   "source": [
    "Now we are ready to perform the generation. The `GenerationSampler` has a `.generate` function that performs the chained generation on a given sample dictionary, following the previously specified generation schedule.\n",
    "It outputs a dictionary that is formatted in the same manner as the sample dictionary, but contains also the predicted tokens. You can change the seed to get different outputs, or set it to None to randomly sample.\n",
    "\n",
    "4M / the sampler outputs discrete tokens, and we still need to decode them to images, feature maps, text, etc using the modality-specific tokenizers. \n",
    "For that, we provide the `decode_dict` function that takes as input the sample dictionary and the tokenizers, and returns plottable representations of each modality.\n",
    "Some modalities like RGB, depth and normals use a diffusion model as the tokenizer decoder. You can specify the number of DDIM steps for decoding with `decoding_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bcacd4-fa76-435c-bffa-2c62cf900b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = sampler.generate(\n",
    "    batched_sample, schedule, text_tokenizer=text_tok, \n",
    "    verbose=True, seed=0,\n",
    "    top_p=top_p, top_k=top_k,\n",
    ")\n",
    "dec_dict = decode_dict(\n",
    "    out_dict, toks, text_tok, \n",
    "    image_size=224, patch_size=16,\n",
    "    decoding_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75faabf0",
   "metadata": {},
   "source": [
    "Load depth retrieval set and tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_retrieval_set_embeddings = torch.load('./retrieval_data/retrieval_set_depth_10k.pth')\n",
    "depth_retrieval_set_tokens = torch.load('./retrieval_data/depth_tokens_10k.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb849d",
   "metadata": {},
   "source": [
    "Given the global embedding predicted for the query and retrieval set, now we can perform retrieval by comparing their cosine distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11396019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index for the retrieval features\n",
    "index = faiss.IndexFlatL2(depth_retrieval_set_embeddings.size(1))\n",
    "index.add(depth_retrieval_set_embeddings.cpu().numpy())\n",
    "\n",
    "query_feature = dec_dict['tok_dinov2_global']\n",
    "query_feature = query_feature.unsqueeze(0).unsqueeze(2).unsqueeze(2).cpu().numpy()\n",
    "\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_feature.reshape(1, -1), k)\n",
    "# The 'indices' variable now contains the indices of the k most similar images\n",
    "\n",
    "# Display the query image\n",
    "plt.figure(figsize=(20,6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 5])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax0.imshow(dec_dict[cond_domains[0]], cmap='gray')\n",
    "ax0.set_title(\"Query Image\")\n",
    "ax0.axis('off')\n",
    "\n",
    "# Display the retrieved images\n",
    "retrieved_images = []\n",
    "for i in indices[0]:\n",
    "    batched_sample = depth_retrieval_set_tokens[i]\n",
    "    retrieved_images.append(decode_tok_depth(batched_sample, toks, 'tok_depth@224'))\n",
    "\n",
    "denormalized_retrieved_images = [torch.from_numpy(img).permute(2,0,1) for img in retrieved_images] \n",
    "retrieved_grid = make_grid(denormalized_retrieved_images, nrow=5)\n",
    "retrieved_grid = retrieved_grid.permute(1, 2, 0)\n",
    "\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.imshow(retrieved_grid)\n",
    "ax1.set_title(\"Retrieved Depth Maps\")\n",
    "ax1.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179c163",
   "metadata": {},
   "source": [
    "Let's do the same to retrieve semantic segmentation maps.\n",
    "First load the retrieval set embeddings and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "semseg_retrieval_set_embeddings = torch.load('./retrieval_data/retrieval_set_semseg_10k.pth')\n",
    "semseg_retrieval_set_tokens = torch.load('./retrieval_data/semseg_tokens_10k.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index for the retrieval features\n",
    "index = faiss.IndexFlatL2(semseg_retrieval_set_embeddings.size(1))\n",
    "index.add(semseg_retrieval_set_embeddings.cpu().numpy())\n",
    "\n",
    "query_feature = dec_dict['tok_dinov2_global']\n",
    "query_feature = query_feature.unsqueeze(0).unsqueeze(2).unsqueeze(2).cpu().numpy()\n",
    "\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_feature.reshape(1, -1), k)\n",
    "# The 'indices' variable now contains the indices of the k most similar images\n",
    "\n",
    "# Display the query image\n",
    "plt.figure(figsize=(20,6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 5])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax0.imshow(dec_dict[cond_domains[0]], cmap='gray')\n",
    "ax0.set_title(\"Query Image\")\n",
    "ax0.axis('off')\n",
    "\n",
    "# Display the retrieved images\n",
    "retrieved_images = []\n",
    "for i in indices[0]:\n",
    "    batched_sample = semseg_retrieval_set_tokens[i]\n",
    "    retrieved_images.append(decode_tok_semseg(np.ones((224, 224, 3)), batched_sample, toks, 'tok_semseg@224'))\n",
    "\n",
    "denormalized_retrieved_images = [torch.from_numpy(img).permute(2,0,1) for img in retrieved_images] \n",
    "retrieved_grid = make_grid(denormalized_retrieved_images, nrow=5)\n",
    "retrieved_grid = retrieved_grid.permute(1, 2, 0)\n",
    "\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.imshow(retrieved_grid)\n",
    "ax1.set_title(\"Retrieved Semantic Segmentation Maps\")\n",
    "ax1.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173a907",
   "metadata": {},
   "source": [
    "### Caption + Metadata → DINOv2 global embedding\n",
    "\n",
    "Alternatively, we can use multimodal input (e.g. a caption and metadata) to perform retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_domains = ['caption', 'metadata']\n",
    "target_domains = ['tok_dinov2_global']\n",
    "tokens_per_target = [16]\n",
    "autoregression_schemes = ['roar']\n",
    "decoding_steps = [1]\n",
    "token_decoding_schedules = [ 'linear' ]\n",
    "temps = [0.1]\n",
    "temp_schedules = ['constant'] \n",
    "cfg_scales = [1.0]\n",
    "cfg_schedules = ['constant'] \n",
    "cfg_grow_conditioning = True\n",
    "top_p, top_k = 0.8, 0.0\n",
    "\n",
    "schedule = build_chained_generation_schedules(\n",
    "    cond_domains=cond_domains, target_domains=target_domains, tokens_per_target=tokens_per_target, autoregression_schemes=autoregression_schemes, \n",
    "    decoding_steps=decoding_steps, token_decoding_schedules=token_decoding_schedules, temps=temps, temp_schedules=temp_schedules,\n",
    "    cfg_scales=cfg_scales, cfg_schedules=cfg_schedules, cfg_grow_conditioning=cfg_grow_conditioning, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = 'hiking in the mountains on a sunny day' \n",
    "metadata_transform = MetadataTransform(shuffle=False, random_trunc=False, return_chunks=False)\n",
    "metadata_dict = {\n",
    "    'semantic_diversity': 2,\n",
    "}\n",
    "metadata_str = metadata_transform.metadata_to_string(metadata_dict) + ' [S_1]'\n",
    "\n",
    "\n",
    "batched_sample = {}\n",
    "\n",
    "# Initialize target modalities\n",
    "for target_mod, ntoks in zip(target_domains, tokens_per_target):\n",
    "    batched_sample = init_empty_target_modality(batched_sample, MODALITY_INFO, target_mod, 1, ntoks, device)\n",
    "\n",
    "batched_sample = custom_text(batched_sample, input_text=caption, text_tokenizer=text_tok, eos_token='[EOS]', key='caption', device=device, target_max_len=256, start_token='[S_1]')\n",
    "batched_sample = custom_text(batched_sample, input_text=metadata_str, text_tokenizer=text_tok, eos_token='[EOS]', key='metadata', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = sampler.generate(\n",
    "    batched_sample, schedule, text_tokenizer=text_tok, \n",
    "    verbose=True, seed=0,\n",
    "    top_p=top_p, top_k=top_k,\n",
    ")\n",
    "dec_dict = decode_dict(\n",
    "    out_dict, toks, text_tok, \n",
    "    image_size=224, patch_size=16,\n",
    "    decoding_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa407e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index for the retrieval features\n",
    "index = faiss.IndexFlatL2(semseg_retrieval_set_embeddings.size(1))\n",
    "index.add(semseg_retrieval_set_embeddings.cpu().numpy())\n",
    "\n",
    "query_feature = dec_dict['tok_dinov2_global']\n",
    "query_feature = query_feature.unsqueeze(0).unsqueeze(2).unsqueeze(2).cpu().numpy()\n",
    "\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_feature.reshape(1, -1), k)\n",
    "# The 'indices' variable now contains the indices of the k most similar images\n",
    "\n",
    "# Display the query image\n",
    "plt.figure(figsize=(20,6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 5])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "metadata_text = ',\\n'.join([f'{k}: {v:.2f}' if isinstance(v, float) else f'{k}: {v}' for k, v in metadata_dict.items()])\n",
    "plot_text_in_square(ax0, \"Query input: \" + caption + \", \" + metadata_text, wrap_width=18, fontsize=14)\n",
    "ax0.axis('off')\n",
    "\n",
    "# Display the retrieved images\n",
    "retrieved_images = []\n",
    "for i in indices[0]:\n",
    "    batched_sample = semseg_retrieval_set_tokens[i]\n",
    "    retrieved_images.append(decode_tok_semseg(np.ones((224, 224, 3)), batched_sample, toks, 'tok_semseg@224'))\n",
    "\n",
    "denormalized_retrieved_images = [torch.from_numpy(img).permute(2,0,1) for img in retrieved_images] \n",
    "retrieved_grid = make_grid(denormalized_retrieved_images, nrow=5)\n",
    "retrieved_grid = retrieved_grid.permute(1, 2, 0)\n",
    "\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.imshow(retrieved_grid)\n",
    "ax1.set_title(\"Retrieved Semantic Segmentation Maps\")\n",
    "ax1.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67210b",
   "metadata": {},
   "source": [
    "Let's increase the semantic diversity of the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "caption = 'hiking in the mountains on a sunny day' \n",
    "metadata_transform = MetadataTransform(shuffle=False, random_trunc=False, return_chunks=False)\n",
    "metadata_dict = {\n",
    "    'semantic_diversity': 20,\n",
    "}\n",
    "metadata_str = metadata_transform.metadata_to_string(metadata_dict) + ' [S_1]'\n",
    "\n",
    "\n",
    "batched_sample = {}\n",
    "\n",
    "# Initialize target modalities\n",
    "for target_mod, ntoks in zip(target_domains, tokens_per_target):\n",
    "    batched_sample = init_empty_target_modality(batched_sample, MODALITY_INFO, target_mod, 1, ntoks, device)\n",
    "\n",
    "batched_sample = custom_text(batched_sample, input_text=caption, text_tokenizer=text_tok, eos_token='[EOS]', key='caption', device=device, target_max_len=256, start_token='[S_1]')\n",
    "batched_sample = custom_text(batched_sample, input_text=metadata_str, text_tokenizer=text_tok, eos_token='[EOS]', key='metadata', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = sampler.generate(\n",
    "    batched_sample, schedule, text_tokenizer=text_tok, \n",
    "    verbose=True, seed=0,\n",
    "    top_p=top_p, top_k=top_k,\n",
    ")\n",
    "dec_dict = decode_dict(\n",
    "    out_dict, toks, text_tok, \n",
    "    image_size=224, patch_size=16,\n",
    "    decoding_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index for the retrieval features\n",
    "index = faiss.IndexFlatL2(semseg_retrieval_set_embeddings.size(1))\n",
    "index.add(semseg_retrieval_set_embeddings.cpu().numpy())\n",
    "\n",
    "query_feature = dec_dict['tok_dinov2_global']\n",
    "query_feature = query_feature.unsqueeze(0).unsqueeze(2).unsqueeze(2).cpu().numpy()\n",
    "\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_feature.reshape(1, -1), k)\n",
    "# The 'indices' variable now contains the indices of the k most similar images\n",
    "\n",
    "# Display the query image\n",
    "plt.figure(figsize=(20,6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 5])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "metadata_text = ',\\n'.join([f'{k}: {v:.2f}' if isinstance(v, float) else f'{k}: {v}' for k, v in metadata_dict.items()])\n",
    "plot_text_in_square(ax0, \"Query input: \" + caption + \", \" + metadata_text, wrap_width=18, fontsize=14)\n",
    "ax0.axis('off')\n",
    "\n",
    "# Display the retrieved images\n",
    "retrieved_images = []\n",
    "for i in indices[0]:\n",
    "    batched_sample = semseg_retrieval_set_tokens[i]\n",
    "    retrieved_images.append(decode_tok_semseg(np.ones((224, 224, 3)), batched_sample, toks, 'tok_semseg@224'))\n",
    "\n",
    "denormalized_retrieved_images = [torch.from_numpy(img).permute(2,0,1) for img in retrieved_images] \n",
    "retrieved_grid = make_grid(denormalized_retrieved_images, nrow=5)\n",
    "retrieved_grid = retrieved_grid.permute(1, 2, 0)\n",
    "\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.imshow(retrieved_grid)\n",
    "ax1.set_title(\"Retrieved Semantic Segmentation Maps\")\n",
    "ax1.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2cd866-ee6f-4284-93a1-65d80bdcb054",
   "metadata": {},
   "source": [
    "# Excercise Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1c864-47da-457c-a6f6-62fe96b463aa",
   "metadata": {},
   "source": [
    "By now, you should have a basic understanding of what multimodal retrieval means and how to use 4M to perform multimodal retrieval.\n",
    "\n",
    "To ensure your comprehension, we have provided two exercises. Follow the instructions below, using code cell blocks for code implementations (ensure they are reproducible without errors) and markdown blocks for explanations and discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e84ff3",
   "metadata": {},
   "source": [
    "## Exercise 1: Design Your Own Multimodal Retrieval Queries [10 Points]\n",
    "\n",
    "1. Design and implement at least **three** different query input approaches to retrieve **semantic segmentation masks that contain cats**. (*3 points for each approach.*)\n",
    "- Each approach should use a different modality or combination of modalities.\n",
    "- For each approach, provide **10** retrieved examples.\n",
    "\n",
    "2. Discussion: Compare the different query inputs. Are the retrieved results similar? What differences do you observe? (*1 point*)\n",
    "\n",
    "Hints: You can use queries based on text, metadata, an image, a semantic segmentation mask, or a combination of these methods. Feel free to explore different text prompts and use any image from the internet for retrieval!\n",
    "\n",
    "**Grading:** To receive full points, ensure your code is reproducible and displays both your query and retrieved images at the end of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145bd62-f723-413b-b3d5-ae057169558d",
   "metadata": {},
   "source": [
    "### Approach 1: (3 points)\n",
    "\n",
    "(Briefly describe your approach here. Which modality or combination of modalities are you using?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3793ea-7177-484a-a845-2b96f3348bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To help structure your answer, we provide a code framework for the first approach. Please complete the missing sections.\n",
    "\n",
    "# 1. Define input and output modalities\n",
    "\n",
    "cond_domains = # [Fill in here]\n",
    "target_domains = ['tok_dinov2_global'] \n",
    "tokens_per_target = [16]\n",
    "autoregression_schemes = ['roar']\n",
    "decoding_steps = [1]\n",
    "token_decoding_schedules = [ 'linear' ]\n",
    "temps = [0.1]\n",
    "temp_schedules = ['constant'] \n",
    "cfg_scales = [1.0]\n",
    "cfg_schedules = ['constant'] \n",
    "cfg_grow_conditioning = True\n",
    "top_p, top_k = 0.8, 0.0\n",
    "\n",
    "schedule = build_chained_generation_schedules(\n",
    "    cond_domains=cond_domains, target_domains=target_domains, tokens_per_target=tokens_per_target, autoregression_schemes=autoregression_schemes, \n",
    "    decoding_steps=decoding_steps, token_decoding_schedules=token_decoding_schedules, temps=temps, temp_schedules=temp_schedules,\n",
    "    cfg_scales=cfg_scales, cfg_schedules=cfg_schedules, cfg_grow_conditioning=cfg_grow_conditioning, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8ac5a-9a2e-4636-8b68-e78a954c8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create your input data\n",
    "\n",
    "batched_sample = {\n",
    "   # [FILL IN HERE]\n",
    "}\n",
    "\n",
    "# Initialize target modalities\n",
    "for target_mod, ntoks in zip(target_domains, tokens_per_target):\n",
    "    batched_sample = init_empty_target_modality(batched_sample, MODALITY_INFO, target_mod, 1, ntoks, device)\n",
    "    \n",
    "# Initialize input modalities \n",
    "    # [FILL IN HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154cc39-6756-44f2-81fd-719195e25332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Get the model prediction\n",
    "\n",
    "out_dict = sampler.generate(\n",
    "    batched_sample, schedule, text_tokenizer=text_tok, \n",
    "    verbose=True, seed=0,\n",
    "    top_p=top_p, top_k=top_k,\n",
    ")\n",
    "dec_dict = decode_dict(\n",
    "    out_dict, toks, text_tok, \n",
    "    image_size=224, patch_size=16,\n",
    "    decoding_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d621d8-7c96-4291-82e3-9d1894966180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Do the retrieval and display\n",
    "index = faiss.IndexFlatL2(semseg_retrieval_set_embeddings.size(1))\n",
    "index.add(semseg_retrieval_set_embeddings.cpu().numpy())\n",
    "\n",
    "query_feature = dec_dict['tok_dinov2_global']\n",
    "query_feature = query_feature.unsqueeze(0).unsqueeze(2).unsqueeze(2).cpu().numpy()\n",
    "\n",
    "k =  # [FILL IN HERE]\n",
    "distances, indices = index.search(query_feature.reshape(1, -1), k)\n",
    "\n",
    "# Display the query image\n",
    "plt.figure(figsize=(20,6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 5])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "metadata_text = ',\\n'.join([f'{k}: {v:.2f}' if isinstance(v, float) else f'{k}: {v}' for k, v in metadata_dict.items()])\n",
    "plot_text_in_square(ax0, \"Query input: \" + caption + \", \" + metadata_text, wrap_width=18, fontsize=14)\n",
    "ax0.axis('off')\n",
    "\n",
    "# Display the retrieved images\n",
    "retrieved_images = []\n",
    "for i in indices[0]:\n",
    "    batched_sample = semseg_retrieval_set_tokens[i]\n",
    "    retrieved_images.append(decode_tok_semseg(np.ones((224, 224, 3)), batched_sample, toks, 'tok_semseg@224'))\n",
    "\n",
    "denormalized_retrieved_images = [torch.from_numpy(img).permute(2,0,1) for img in retrieved_images] \n",
    "retrieved_grid = make_grid(denormalized_retrieved_images, nrow=5)\n",
    "retrieved_grid = retrieved_grid.permute(1, 2, 0)\n",
    "\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.imshow(retrieved_grid)\n",
    "ax1.set_title(\"Retrieved Semantic Segmentation Maps\")\n",
    "ax1.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52190ac2-c534-4e18-be10-75856d0f9010",
   "metadata": {},
   "source": [
    "### Approach 2 (3 points)\n",
    "(Briefly describe your approach here. Which modality or combination of modalities are you using?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927a47e-a558-4636-a58f-30b04f6f6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please provide your code below, you may use multiple cells\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6d46f-7123-420d-9ab0-697dd96ef28a",
   "metadata": {},
   "source": [
    "### Approach 3 (3 points)\n",
    "(Briefly describe your approach here. Which modality or combination of modalities are you using?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a52b9-d57b-4d9c-aab6-9bf74cc46278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please provide your code below, you may use multiple cells\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781abffa-4e82-46d2-b16d-a08abd4e2f30",
   "metadata": {},
   "source": [
    "### Discussion (1 point)\n",
    "Compare the different query inputs. Are the retrieved results similar? What differences do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6a8af-c09f-45de-b1f3-9ec9369afd64",
   "metadata": {},
   "source": [
    "(Provide your answer here.)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d2cdf",
   "metadata": {},
   "source": [
    "## Exercise 2: Using Alternative Deep Features for Retrieval [10 Points]\n",
    "\n",
    "Beyond DINOv2, the 4M model also supports feature outputs from ImageBind. For this task, use ImageBind to retrieve semantic segmentation masks containing cats from the 10 semseg examples provided below.\n",
    "\n",
    "Hints:\n",
    "- Go through the 10 semantic segmentation examples, use the 4M model to get their corresponding deep features, and use them for retrieval.\n",
    "- For simplicity, feel free to choose any basic input modality, such as text, image, or any other modality you prefer, to complete the task.\n",
    "\n",
    "**Grading:** To achieve a full score, ensure your code is reproducible and that the final output displays three semantic segmentation images of cats within the given subset (see next block). A 2-point deduction will be applied for each missing cat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d8dc2-6aca-4b88-bbb4-6a5e4551a75d",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1418a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Please run the code below and review the subset of semantic segmentation tokens.\n",
    "\n",
    "# The subset of indices for semantic segmentation retrieval\n",
    "semseg_retrieval_subset_id = [2, 394, 376, 800, 1046, 5000, 6559, 7201, 8945, 9001]\n",
    "semseg_retrieval_subset_tokens = []\n",
    "for i in semseg_retrieval_subset_id:\n",
    "    semseg_retrieval_subset_tokens.append(semseg_retrieval_set_tokens[i])\n",
    "\n",
    "# Let's visualize the subset\n",
    "retrieved_images = []\n",
    "for token in semseg_retrieval_subset_tokens:\n",
    "    retrieved_images.append(decode_tok_semseg(np.ones((224, 224, 3)), token, toks, 'tok_semseg@224'))\n",
    "\n",
    "denormalized_retrieved_images = [torch.from_numpy(img).permute(2,0,1) for img in retrieved_images] \n",
    "retrieved_grid = make_grid(denormalized_retrieved_images, nrow=5)\n",
    "retrieved_grid = retrieved_grid.permute(1, 2, 0)\n",
    "\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 5])\n",
    "plt.figure(figsize=(20,6))\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.imshow(retrieved_grid)\n",
    "ax1.set_title(\"Task: Use alternative deep features to find semantic segmentation maps containing cats in the 10 examples.\")\n",
    "ax1.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d42751-d487-4112-b586-b40b493e9f01",
   "metadata": {},
   "source": [
    "# Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d3ad1-a2cb-46f9-854f-758ff5d0893c",
   "metadata": {},
   "source": [
    "We provide the partial code below. Please read it carefully and follow the instructions to complete the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce24b1-a3f7-4034-99f6-aef82fccb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load the semseg and imagebind tokenizers\n",
    "toks = {\n",
    "    'tok_semseg': VQVAE.from_pretrained('EPFL-VILAB/4M_tokenizers_semseg_4k_224-448').eval().to(device),\n",
    "    'tok_imagebind_global': VQVAE.from_pretrained('EPFL-VILAB/4M_tokenizers_ImageBind-H14-global_8k_16_224').eval().to(device)\n",
    "    # ...\n",
    "    # Add any other modality you may need for later use\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694d79f-f70f-4689-94ff-a9314bd21392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Iterate through the 10 semantic segmentation images and extract their ImageBind features.\n",
    "\n",
    "cond_domains = # [FILL IN HERE]\n",
    "target_domains = # [FILL IN HERE]\n",
    "\n",
    "tokens_per_target = [16]\n",
    "autoregression_schemes = ['roar']\n",
    "decoding_steps = [1]\n",
    "token_decoding_schedules = ['linear']\n",
    "temps = [0.1]\n",
    "temp_schedules = ['constant'] \n",
    "cfg_scales = [1.0]\n",
    "cfg_schedules = ['constant'] \n",
    "cfg_grow_conditioning = True\n",
    "top_p, top_k = 0.8, 0.0\n",
    "\n",
    "schedule = build_chained_generation_schedules(\n",
    "    cond_domains=cond_domains, target_domains=target_domains, tokens_per_target=tokens_per_target, autoregression_schemes=autoregression_schemes, \n",
    "    decoding_steps=decoding_steps, token_decoding_schedules=token_decoding_schedules, temps=temps, temp_schedules=temp_schedules,\n",
    "    cfg_scales=cfg_scales, cfg_schedules=cfg_schedules, cfg_grow_conditioning=cfg_grow_conditioning, \n",
    ")\n",
    "\n",
    "semseg_retrieval_set_embeddings_imagebind = []\n",
    "\n",
    "\n",
    "########## [YOUR ANSWER BEGINS] #########\n",
    "# Extract the ImageBind features for the subset tokens and store them in semseg_retrieval_set_embeddings_imagebind.\n",
    "\n",
    "for token in semseg_retrieval_subset_tokens:\n",
    "    # ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "########## [YOUR ANSWER ENDS] #########\n",
    "    \n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(semseg_retrieval_set_embeddings_imagebind.size(1))\n",
    "index.add(semseg_retrieval_set_embeddings_imagebind.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070443fd-7baa-4e4b-a846-bca535fa35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the query input and extract the corresponding ImageBind features.\n",
    "\n",
    "cond_domains = #[FILL IN HERE]\n",
    "target_domains = #[FILL IN HERE]\n",
    "tokens_per_target = [16]\n",
    "autoregression_schemes = ['roar']\n",
    "decoding_steps = [1]\n",
    "token_decoding_schedules = [ 'linear' ]\n",
    "temps = [0.1]\n",
    "temp_schedules = ['constant'] \n",
    "cfg_scales = [1.0]\n",
    "cfg_schedules = ['constant'] \n",
    "cfg_grow_conditioning = True\n",
    "top_p, top_k = 0.8, 0.0\n",
    "\n",
    "schedule = build_chained_generation_schedules(\n",
    "    cond_domains=cond_domains, target_domains=target_domains, tokens_per_target=tokens_per_target, autoregression_schemes=autoregression_schemes, \n",
    "    decoding_steps=decoding_steps, token_decoding_schedules=token_decoding_schedules, temps=temps, temp_schedules=temp_schedules,\n",
    "    cfg_scales=cfg_scales, cfg_schedules=cfg_schedules, cfg_grow_conditioning=cfg_grow_conditioning, \n",
    ")\n",
    "\n",
    "########## [YOUR ANSWER BEGINS] #########\n",
    "# Design the query input and get the corresponding Imagebind feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query_feature = \n",
    "########## [YOUR ANSWER ENDS] #########\n",
    "\n",
    "# Do retrieval based on the \n",
    "k =  3\n",
    "distances, indices = index.search(query_feature.reshape(1, -1), k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b6cda-49db-4615-91a7-6f7d21595d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualization\n",
    "\n",
    "# Display the query \n",
    "plt.figure(figsize=(20,6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 5])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "# [(OPTIONAL) DISPLAY YOUR QUERY INPUT HERE]\n",
    "# The query visualization is not graded, but it will make your TA happy by clearly showcasing your method. : )\n",
    "\n",
    "ax0.axis('off')\n",
    "\n",
    "# Display the retrieved images\n",
    "retrieved_images = []\n",
    "for i in indices[0]:\n",
    "    batched_sample = semseg_retrieval_subset_tokens[i]\n",
    "    retrieved_images.append(decode_tok_semseg(np.ones((224, 224, 3)), batched_sample, toks, 'tok_semseg@224'))\n",
    "\n",
    "denormalized_retrieved_images = [torch.from_numpy(img).permute(2,0,1) for img in retrieved_images] \n",
    "retrieved_grid = make_grid(denormalized_retrieved_images, nrow=5)\n",
    "retrieved_grid = retrieved_grid.permute(1, 2, 0)\n",
    "\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.imshow(retrieved_grid)\n",
    "ax1.set_title(\"Retrieved Semantic Segmentation Maps\")\n",
    "ax1.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
